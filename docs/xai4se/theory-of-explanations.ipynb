{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A Theory of Explanations\n",
    "    \n",
    "## A Theory of Explainability\n",
    "\n",
    "According to philosophy, social science, and psychology theories, a\n",
    "common definition of *explainability or interpretability* is *the degree\n",
    "to which a human can understand the reasons behind a decision or an\n",
    "action* {cite}`Miller19`. The explainability of AI/ML algorithms can be\n",
    "achieved by (1) making the entire decision-making process transparent\n",
    "and comprehensible and (2) explicitly providing an explanation for each\n",
    "decision {cite}`lipton2018mythos` (since an explanation is not likely applicable to\n",
    "all decisions {cite}`leake2014evaluating`. Hence, research has emerged to\n",
    "explore how to explain decisions made by complex, black-box models and\n",
    "how explanations are presented in a form that would be easily understood\n",
    "(and hence, accepted) by humans.\n",
    "\n",
    "\n",
    "## Explainability Goals\n",
    "\n",
    "**_Please check this subsection._**\n",
    "\n",
    "Explanations of AI/ML systems can be presented in various forms to serve various goals of different stakeholders.\n",
    "For example, the goal of software managers is to understand the general characteristics of software projects that are associated with defect-proneness to chart appropriate qualitity improvement plans.\n",
    "Thus, explanations needed by such software managers are generic explanations that describe the whole reasons and logic behind decisions of AI/ML systems.\n",
    "On the other hand, the goal of software developers is to understand why a particular file is predicted as defective by AI/ML systems to mitigate the risk of such file being defective.\n",
    "In this case, explanations needed by software developers are explanations that are specific to such file and describe the reasons and logic behind defective predictions made by AI/ML systems.\n",
    "\n",
    "<!-- \n",
    "TODO?\n",
    "Different stakeholders have different goals, thus require different forms of explanations. -->\n",
    "\n",
    "## What are Explanations?\n",
    "\n",
    "According to a philosophical and psychological theory of explanations,\n",
    "Salmon  {cite}`Salmon84` argue that explanations can be presented as a causal\n",
    "chain of causes that lead to the decision. Causal chains can be\n",
    "classified into five categories {cite}`Hilton2005`: temporal, coincidental,\n",
    "unfolding, opportunity chains and pre-emptive. Each type of causal chain\n",
    "is thus associated with an explanation type. However, identifying the\n",
    "complete causal chain of causes is challenging, since most AI/ML\n",
    "techniques produce only correlations instead of causations.\n",
    "In contrast, Miller {cite}`Miller19` argue that explanations can be presented\n",
    "as answers to why-questions. Similarly, Lipton {cite}`lipton1990contrastive` also share\n",
    "a similar view of explanations as being contrastive. \n",
    "\n",
    "\n",
    "### Types of Intelligibility Questions\n",
    "\n",
    "**_Please check this subsection._**\n",
    "\n",
    "Lim and Dey categorized questions towards the inference mechanism of AI/ML systems into: What, Why, Why Not, What If, and How To.\n",
    "While they initially found that Why and Why Not explanations were most effective in promoting system understanding and trust [65],\n",
    "they later found that users use different strategies to check model behavior and thus use different intelligibility queries for the same interpretability goals [68, 70]. \n",
    "Below, we provide an example of questions for each type of the intelligibility questions.\n",
    "- *What*: What is the logic behind the AI/ML models?\n",
    "- *Why*: Why is an instance predicted as TRUE?\n",
    "- *Why Not*: Why is an instance predicted as TRUE?\n",
    "- *What If*: What would the system predict if the values of an instance are changed?\n",
    "- *How To*: How can we reverse the prediction of an instance (e.g., from TRUE to FALSE) generated by the system?\n",
    "\n",
    "### Scopes of Explanations\n",
    "\n",
    "Explainability can be achieved at two levels:\n",
    "\n",
    "The explainability of software analytics can be achieved by:\n",
    "\n",
    "-   **Global Explanability:** Using interpretable machine learning\n",
    "    techniques (e.g., decision tree, decision rules or logistic\n",
    "    regression techniques) or intrinsic model-specific techniques (e.g.,\n",
    "    ANOVA, variable importance) so the entire predictions and\n",
    "    recommendations process are transparent and comprehensible. Such\n",
    "    intrinsic model-specific techniques aim to provide the global\n",
    "    explainability. Thus, users can only understand how the model works\n",
    "    globally (e.g., by inspecting a branch of decision trees). However,\n",
    "    users often do not understand why the model makes that prediction.\n",
    "\n",
    "-   **Local Explanability:** Using model-agnostic techniques (e.g.,\n",
    "    LIME {cite}`ribeiro2016should`) to explain the predictions of the\n",
    "    software analytics models (e.g., neural network, random forest).\n",
    "    Such post-hoc model-agnostic techniques can provide an explanation\n",
    "    for each prediction (i.e., an instance to be explained). Users can\n",
    "    then understand why the prediction is made by the software analytics\n",
    "    models.\n",
    "\n",
    "\n",
    "### Types of Explanations\n",
    "\n",
    "There are so many types of explanations in which we summarise into x types.\n",
    "WHO SAID??\n",
    "THEN WHAT ARE THOSE?\n",
    "- Plain,\n",
    "- Contrast,\n",
    "-- O-contrast\n",
    "-- P-contrast\n",
    "-- T-contrast\n",
    "- counterfactual, \n",
    "- causal.\n",
    "\n",
    "<!-- \n",
    "Lim and Dey categorized questions towards the inference mechanism of AI/ML systems into: What, Why, Why Not, What If, and How To.\n",
    "While they initially found that Why and Why Not explanations were most effective in promoting system understanding and trust [65],\n",
    "they later found that users use different strategies to check model behavior and thus use different intelligibility queries for the same interpretability goals [68, 70].  -->\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Finally, we present a summary table of the scope of explanations, types of explanation, definition, examples, and techniques for generating answers towards such questions with respect to the five types of questions.\n",
    "\n",
    "<!-- \n",
    "\n",
    "five types of questions as well as their examples and the techniques used to generate explanations towards them:\n",
    "\n",
    "- **What**\n",
    "\n",
    "While a reasoning trace typically addresses the question of\n",
    "why and how the system did something, there are\n",
    "several other questions that end-users of novel systems may\n",
    "ask. We chose to look into the following questions (adapted\n",
    "from [11]):\n",
    "1. What: What did the system do?\n",
    "2. Why: Why did the system do W?\n",
    "3. Why Not: Why did the system not do X?\n",
    "4. What If: What would the system do if Y happens?\n",
    "5. How To: How can I get the system to do Z, given the\n",
    "current context?\n",
    "\n",
    " -->\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "-   **Plain-fact** is the properties of the object. *Why does object $a$ have property $P$?*\n",
    "    \\\n",
    "    Example: Why is file $A$ defective?\n",
    "-   **Property-contrast** is the differences in the Properties within an object. *Why does object $a$ have property $P$, rather than property $P'$?*\n",
    "    \\\n",
    "    Example:\n",
    "```{figure} /xai4se/images/p-contrast.png\n",
    "---\n",
    "name: p-contrast\n",
    "---\n",
    "An illustrative example of P-contrast explanation.\n",
    "``` \n",
    "-   **Object-contrast** is the differences between two Objects. *Why does object $a$ have property $P$, while object $b$ has property $P'$?*\n",
    "    \\\n",
    "    Example:\n",
    "    \n",
    "```{figure} /xai4se/images/o-contrast.png\n",
    "---\n",
    "name: o-contrast\n",
    "---\n",
    "An illustrative example of O-contrast explanation.\n",
    "``` \n",
    " \n",
    "-   **Time-contrast** is the differences within an object over Time. *Why does object $a$ have property $P$ at time $t$, but property $P'$ at\n",
    "    time $t'$?*\n",
    "    \\\n",
    "    Example:\n",
    "    \n",
    "```{figure} /xai4se/images/t-contrast.png\n",
    "---\n",
    "name: t-contrast\n",
    "---\n",
    "An illustrative example of T-contrast explanation.\n",
    "``` \n",
    " \n",
    "Answers to the P-contrast, O-contrast and T-contrast why-questions form\n",
    "an explanation. *Contrastive explanations focus on only the differences\n",
    "on **Properties within an object** (Property-contrast), between **two\n",
    "Objects** (Object-contrast), and **within an object over Time**\n",
    "(Time-contrast)* {cite}`VanBouwel2002`. Answering a plain fact question is\n",
    "generally more difficult than generating answers to the contrastive\n",
    "questions {cite}`lipton1990contrastive`. For example, we could answer the\n",
    "Property-contrast question (e.g., \"Why is file $A$ classified as being\n",
    "defective instead of being clean?\") by citing that there are a\n",
    "substantial number of defect-fixing commits that involve with the file.\n",
    "Information about the size, complexity, owner of the file, and so on are\n",
    "not required to answer this question. On the other hand, explaining why\n",
    "file $A$ is defective in a non-contrastive manner would require us to\n",
    "use all causes. In addition, humans tend to be cognitively attached to\n",
    "digest contrastive explanations {cite}`Miller19`. Thus, contrastive\n",
    "explanations may be more valuable and more intuitive to humans. These\n",
    "important factors from both social and computational perspectives should\n",
    "be considered when providing explainable models or tool support for\n",
    "software engineering.\n",
    "\n",
    "Explanation is not only a *product*, as discussed above, but also a\n",
    "*process* {cite}`Lombrozo2006`. In fact, generating explanations is a\n",
    "*cognitive process* which essentially involves four cognitive systems:\n",
    "(1) attention, (2) long-term memory, (3) working memory, and (4)\n",
    "metacognition {cite}`Horne2019`{cite}`leake2014evaluating`. Recent\n",
    "work {cite}`Miller19` further recognised the importance of considering\n",
    "explanation as being not only a cognitive process but also a *social\n",
    "process*, in which an explainer communicates knowledge to an explainee.\n",
    "Using this view, explanations should be considered as part of a\n",
    "conversation between the explainer and explainee. The theories, models,\n",
    "and processes of how humans explain decisions to one another are\n",
    "important to the work on explainable software analytics and the\n",
    "development of explainable tool support for software engineering in\n",
    "general.\n",
    "<!-- \n",
    "## Explanation's Goals\n",
    "\n",
    "asdfasdfsadf\n",
    "\n",
    "## Intelligibility Questions\n",
    "\n",
    "## Structure of Explanations\n",
    "\n",
    "## Visualization of Explanations\n",
    " -->\n",
    "\n",
    "<!-- \n",
    "\\subsubsection{Inquiry and Reasoning}\n",
    "With the various goals of\n",
    "explanations, the user would then seek to find causes or\n",
    "generalize their knowledge and reason about the\n",
    "information or explanations received. Pierce defined three\n",
    "kinds of inferences [83]: deduction, induction, and\n",
    "abduction. Deductive reasoning “top-down logic” is the process of reasoning from premises to a conclusion.\n",
    "Inductive reasoning “bottom-up logic” is the reverse\n",
    "process of reasoning from a single observation or instance\n",
    "to a probable explanation or generalization. Abductive\n",
    "reasoning is also the reverse of deductive reasoning and\n",
    "reasons from an observation to the most likely explanation.\n",
    "This is also known as “inference to the best explanation”. It\n",
    "is more selective than inductive reasoning, since it\n",
    "prioritizes hypotheses.\n",
    "\n",
    "<!--  -->\n",
    "\\subsubsection{Types of Explanation} \n",
    "% Causal Attribution and Explanations\n",
    "As users inquire\n",
    "for more information to understand an observation, they\n",
    "may seek different types of explanations. Miller identified\n",
    "causal explanations as a key type of explanation, but also\n",
    "distinguished them from causal attribution, and non-causal\n",
    "explanations [79].\n",
    "Causal attribution refers to the articulation of internal or\n",
    "external factors that could be attributed to influence the\n",
    "outcome or observation [37]. Miller argues that this is not\n",
    "strictly a causal explanation, since it does not precisely\n",
    "identify key causes. Nevertheless, they provide broad\n",
    "information from which users can judge and identify\n",
    "potential causes. Combining attribution across time and\n",
    "sequence would lead to a causal chain, which is\n",
    "considered a trace explanation or line of reasoning.\n",
    "Causal explanation refers to an explanation that is\n",
    "focused on selected causes relevant to interpreting the\n",
    "observation with respect to existing knowledge. This\n",
    "requires that the explanation be contrastive between a fact\n",
    "(what happened) and a foil (what is expected or plausible to\n",
    "happen) [71, 79]. Users can ask why not to understand why\n",
    "a foil did not happen. The selected subset of causes thus\n",
    "provides a counterfactual explanation of what needs to\n",
    "change for the alternative outcome to happen. This helps\n",
    "people to identify causes, on the scientific principle that\n",
    "manipulating a cause will change the effect. This also\n",
    "provides a more usable explanation than causal attribution,\n",
    "because it presents fewer factors (reduces information\n",
    "overload) and can provide users with a greater perception\n",
    "of control, i.e., how to control the system. A similar method\n",
    "is to ask what if the factors were different, then what the\n",
    "effect would be. Since this asks about prospective future\n",
    "behavior, Hoffman and Klein calls this transfactual\n",
    "reasoning; conversely, counterfactual reasoning asks\n",
    "retrospectively [40, 41]. This articulation highlights the\n",
    "importance of contrastive (Why Not) and counterfactual\n",
    "(How To) explanations instead of simple trace or\n",
    "attribution explanations typically used for transparency\n",
    "\n",
    "\\subsection{Designing Explanations}\n",
    "\n",
    "\\subsubsection{intelligibility queries:}\n",
    "% inputss\n",
    "% outputs\n",
    "% certainty\n",
    "%H ow\n",
    "% why\n",
    "% why not, \n",
    "% what if,\n",
    "% how to?\n",
    "Lim and Dey identified several\n",
    "queries (called intelligibility queries) that a user may ask of\n",
    "a smart system [66, 67]. Starting from a usability-centric\n",
    "perspective, the authors developed a suite of colloquial\n",
    "questions about the system state (Inputs, What Output,\n",
    "What Else Outputs, Certainty), and inference mechanism\n",
    "(Why, Why Not, What If, How To). While they initially\n",
    "found that Why and Why Not explanations were most\n",
    "effective in promoting system understanding and trust [65],\n",
    "they later found that users use different strategies to check\n",
    "model behavior and thus use different intelligibility queries\n",
    "for the same interpretability goals [68, 70]. In this work, we\n",
    "identify theoretical foundations that justify these different\n",
    "queries.\n",
    "The previous subsections described mechanisms and\n",
    "constructs driven by reasoning semantics and explanation\n",
    "goals. These generate explanations that are often in similar\n",
    "forms. Next, we describe common data structures and\n",
    "atomic elements of explanations that are used to represent\n",
    "these semantic structures.\n",
    "\n",
    "While a reasoning trace typically addresses the question of\n",
    "why and how the application did something, there are\n",
    "several other questions that end-users of novel systems may\n",
    "ask. We chose to look into the following questions (adapted\n",
    "from [11]):\n",
    "1. What: What did the system do?\n",
    "2. Why: Why did the system do W?\n",
    "3. Why Not: Why did the system not do X?\n",
    "4. What If: What would the system do if Y happens?\n",
    "5. How To: How can I get the system to do Z, given the\n",
    "current context?\n",
    "\n",
    "\n",
    "\\subsubsection{Structure of Explanations}\n",
    "\n",
    "The simplest\n",
    "and most common way to construct explanations is as lists\n",
    "If sorted, this would represent concepts\n",
    "related by some criteria of importance. Lists are often used\n",
    "to represent input feature attributions, and can also\n",
    "represent output class attributions. Logical clauses can be\n",
    "combined into rules [63] or as a decision tree to describe\n",
    "branching logic. \n",
    "\n",
    "\n",
    "\n",
    "\\subsubsection{Visualization of Explanations}\n",
    "While some explanation structures can\n",
    "be communicated with textual explanations or single lists,\n",
    "complex concepts are better explained with visualization\n",
    "techniques. To provide data and algorithmic transparency,\n",
    "basic charts can be used to represent raw data (e.g., line\n",
    "charts for time series data) and canonical visualizations\n",
    "can be used to illustrate model structure (e.g., node-link\n",
    "diagrams for trees and graphs). To support causal\n",
    "attribution, tornado diagrams of vertical bar charts are\n",
    "popularly used for lists of attributions [86], saliency\n",
    "heatmaps for attributing to pixels or super-pixels for\n",
    "image-based models [50, 86], or highlighting on paragraph\n",
    "text [15, 58]. These visualization techniques support\n",
    "contrastive explanations and counterfactual reasoning by\n",
    "allowing for the comparison of different attributions (e.g.,\n",
    "bar charts, heatmaps) or understanding of relationships\n",
    "between factors (tree or graph visualizations). Leveraging\n",
    "on inductive reasoning, scatterplot diagrams help users\n",
    "to perceive the similarity between objects by presenting\n",
    "objects in lower dimensionality projection (e.g., t-SNE)\n",
    "[47]. Drawing from statistical inference, partial\n",
    "dependence (PD) plots have been used to visualize how\n",
    "feature attribution varies across different feature values [16,\n",
    "56]. Some work has extended this to show interaction\n",
    "effects between two factors [16, 75]. Finally, sensitivity\n",
    "analysis provides several methods as a robustness test to\n",
    "ask what if the input factors change slightly or are\n",
    "perturbed, whether the outcome of a decision will change.\n",
    "These visualization techniques support rational choice\n",
    "reasoning with counterfactuals by displaying how the\n",
    "expected utility or risk of an outcome changes as input\n",
    "factors change. Hence, these decision aids, if used properly\n",
    "and deliberately, can support rational decision making. -->\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "```{note}\n",
    "Parts of this chapter have been published by Jirayus Jiarpakdee, Chakkrit Tantithamthavorn, Hoa Khanh Dam, John Grundy: An Empirical Study of Model-Agnostic Techniques for Defect Prediction Models. IEEE Trans. Software Eng. (2020) https://doi.org/10.1109/TSE.2020.2982385\"\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "xaitools",
   "language": "python",
   "name": "xaitools"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
