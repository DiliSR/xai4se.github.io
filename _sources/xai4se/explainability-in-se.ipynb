{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Explainability in Software Engineering\n",
    "\n",
    "Software engineering is by nature a collaborative social practice.\n",
    "Collaboration among different stakeholders (e.g., users, developers, and\n",
    "managers) is essential in modern software engineering. As a part of the\n",
    "collaboration, individuals are often expected to explain decisions made\n",
    "throughout software development processes to develop appropriate trust\n",
    "and enable effective communication. Since tool support in software\n",
    "development processes is an integral part of this collaborative process,\n",
    "similar expectations are also applied. Such tools should not only\n",
    "provide insights or generate predictions for recommendation, but also be\n",
    "able to explain such insights and recommendations.\n",
    "\n",
    "Recent automated and advanced software development tools heavily rely on\n",
    "Artificial Intelligence and Machine Learning (AI/ML) capabilities to\n",
    "predict software defects, estimate development effort, and recommend API\n",
    "choices. However, such AI/ML algorithms are often \"black-box\", which\n",
    "makes it hard for practitioners to understand how the models arrive at a\n",
    "decision. A lack of explainability of the black-box algorithms leads to\n",
    "a lack of trust in the predictions or recommendations produced by such\n",
    "algorithms.\n",
    "\n",
    "\n",
    "## Explainable AI for SE: A Way Forward\n",
    "\n",
    "Explainable AI is a suite of AI/ML techniques that produce accurate\n",
    "predictions, while being able to explain such predictions. The purpose\n",
    "of increasing the explainability of software analytics (XAI4SE) is to\n",
    "make its behavior more intelligible to humans by providing explanations.\n",
    "The explainability of software analytics can be achieved by:\n",
    "\n",
    "-   **\"Global Explanability\\\":** Using interpretable machine learning\n",
    "    techniques (e.g., decision tree, decision rules or logistic\n",
    "    regression techniques) or intrinsic model-specific techniques (e.g.,\n",
    "    ANOVA, variable importance) so the entire predictions and\n",
    "    recommendations process are transparent and comprehensible. Such\n",
    "    intrinsic model-specific techniques aim to provide the global\n",
    "    explainability. Thus, users can only understand how the model works\n",
    "    globally (e.g., by inspecting a branch of decision trees). However,\n",
    "    users often do not understand why the model makes that prediction.\n",
    "\n",
    "-   **\"Local Explanability\\\":** Using model-agnostic techniques (e.g.,\n",
    "    LIME {cite}`ribeiro2016should`) to explain the predictions of the\n",
    "    software analytics models (e.g., neural network, random forest).\n",
    "    Such post-hoc model-agnostic techniques can provide an explanation\n",
    "    for each prediction (i.e., an instance to be explained). Users can\n",
    "    then understand why the prediction is made by the software analytics\n",
    "    models.\n",
    "    \n",
    "    \n",
    "## A Theory of Explainability\n",
    "\n",
    "According to philosophy, social science, and psychology theories, a\n",
    "common definition of *explainability or interpretability* is *the degree\n",
    "to which a human can understand the reasons behind a decision or an\n",
    "action* {cite}`Miller19`. The explainability of AI/ML algorithms can be\n",
    "achieved by (1) making the entire decision-making process transparent\n",
    "and comprehensible and (2) explicitly providing an explanation for each\n",
    "decision {cite}`lipton2018mythos` (since an explanation is not likely applicable to\n",
    "all decisions {cite}`leake2014evaluating`. Hence, research has emerged to\n",
    "explore how to explain decisions made by complex, black-box models and\n",
    "how explanations are presented in a form that would be easily understood\n",
    "(and hence, accepted) by humans.\n",
    "\n",
    "## A Theory of Explanations\n",
    "\n",
    "\n",
    "According to a philosophical and psychological theory of explanations,\n",
    "Salmon  {cite}`Salmon84` argue that explanations can be presented as a causal\n",
    "chain of causes that lead to the decision. Causal chains can be\n",
    "classified into five categories {cite}`Hilton2005`: temporal, coincidental,\n",
    "unfolding, opportunity chains and pre-emptive. Each type of causal chain\n",
    "is thus associated with an explanation type. However, identifying the\n",
    "complete causal chain of causes is challenging, since most AI/ML\n",
    "techniques produce only correlations instead of causations.\n",
    "\n",
    "In contrast, Miller {cite}`Miller19` argue that explanations can be presented\n",
    "as answers to why-questions. Similarly, Lipton {cite}`lipton1990contrastive` also share\n",
    "a similar view of explanations as being *contrastive*. There are three\n",
    "components of why-questions {cite}`van1980scientific`: (1) the event to be explained,\n",
    "also called the *explanandum* (e.g., file A is defective); (2) a set of\n",
    "similar events that are similar to the explanandum but did not occur\n",
    "(e.g., file A is clean); and (3) a request for information that can\n",
    "distinguish the occurrence of the explanandum from the non-occurrence of\n",
    "the other similar events (e.g., a large number of changes made to file\n",
    "A). Below, we describe four types of why-questions:\n",
    "\n",
    "-   **Plain-fact** is the properties of the object. *Why does object $a$ have property $P$?*\n",
    "    \\\n",
    "    Example: Why is file $A$ defective?\n",
    "-   **Property-contrast** is the differences in the Properties within an object. *Why does object $a$ have property $P$, rather than property $P'$?*\n",
    "    \\\n",
    "    Example: Why is file $A$ defective rather than clean?\n",
    "-   **Object-contrast** is the differences between two Objects. *Why does object $a$ have property $P$, while object $b$ has property $P'$?*\n",
    "    \\\n",
    "    Example: Why is file $A$ defective, while file $B$ is clean?\n",
    "-   **Time-contrast** is the differences within an object over Time. *Why does object $a$ have property $P$ at time $t$, but property $P'$ at\n",
    "    time $t'$?*\n",
    "    \\\n",
    "    Example: Why was file $A$ not classified as defective in version 1.2, but was subsequently classified as defective in version 1.3?    \n",
    " \n",
    "Answers to the P-contrast, O-contrast and T-contrast why-questions form\n",
    "an explanation. *Contrastive explanations focus on only the differences\n",
    "on **Properties within an object** (Property-contrast), between **two\n",
    "Objects** (Object-contrast), and **within an object over Time**\n",
    "(Time-contrast)* {cite}`VanBouwel2002`. Answering a plain fact question is\n",
    "generally more difficult than generating answers to the contrastive\n",
    "questions {cite}`lipton1990contrastive`. For example, we could answer the\n",
    "Property-contrast question (e.g., \"Why is file $A$ classified as being\n",
    "defective instead of being clean?\") by citing that there are a\n",
    "substantial number of defect-fixing commits that involve with the file.\n",
    "Information about the size, complexity, owner of the file, and so on are\n",
    "not required to answer this question. On the other hand, explaining why\n",
    "file $A$ is defective in a non-contrastive manner would require us to\n",
    "use all causes. In addition, humans tend to be cognitively attached to\n",
    "digest contrastive explanations {cite}`Miller19`. Thus, contrastive\n",
    "explanations may be more valuable and more intuitive to humans. These\n",
    "important factors from both social and computational perspectives should\n",
    "be considered when providing explainable models or tool support for\n",
    "software engineering.\n",
    "\n",
    "Explanation is not only a *product*, as discussed above, but also a\n",
    "*process* {cite}`Lombrozo2006`. In fact, generating explanations is a\n",
    "*cognitive process* which essentially involves four cognitive systems:\n",
    "(1) attention, (2) long-term memory, (3) working memory, and (4)\n",
    "metacognition {cite}`Horne2019`{cite}`leake2014evaluating`. Recent\n",
    "work {cite}`Miller19` further recognised the importance of considering\n",
    "explanation as being not only a cognitive process but also a *social\n",
    "process*, in which an explainer communicates knowledge to an explainee.\n",
    "Using this view, explanations should be considered as part of a\n",
    "conversation between the explainer and explainee. The theories, models,\n",
    "and processes of how humans explain decisions to one another are\n",
    "important to the work on explainable software analytics and the\n",
    "development of explainable tool support for software engineering in\n",
    "general."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```{note}\n",
    "Parts of this chapter have been published by Jirayus Jiarpakdee, Chakkrit Tantithamthavorn, Hoa Khanh Dam, John Grundy: An Empirical Study of Model-Agnostic Techniques for Defect Prediction Models. IEEE Trans. Software Eng. (2020) https://doi.org/10.1109/TSE.2020.2982385\"\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "xaitools",
   "language": "python",
   "name": "xaitools"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
